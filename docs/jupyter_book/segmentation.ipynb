{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(segmentation)=\n",
    "# Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pyKNEEr* computes **atlas-based segmentation**, which is based on **registration**, using *elastix* [{ref}`1<klein>`].\n",
    "In registration there are a **reference** (or target) image and a **moving** (or floating) image, which is warped to the reference image.\n",
    "The reference image is already segmented, whereas the moving image has to be segmented.\n",
    "\n",
    "In *pyKNEEr*, atlas-based segmentation has three steps:\n",
    "\n",
    "1. The moving image is registered to the reference image though transformations\n",
    "2. The transformations are inverted\n",
    "3. The inverted transformations are applied to the reference mask to obtain the moving mask\n",
    "\n",
    "These steps are applied:\n",
    "\n",
    "1. To the **femur** to align the moving image to the reference image, and guide femoral cartilage segmentation\n",
    "2. To the **femoral cartilage** to obtain the segmented image\n",
    "\n",
    "``` {Important}\n",
    "\n",
    "   The current method does **not** take into account segmentation accuracy at the **bone-cartilage interface**.\n",
    "   The femur alignment is used to guide cartilage segmentation, and femur segmentation is a byproduct of the workflow.\n",
    "```\n",
    "   \n",
    "\n",
    "In *pyKNEEr*, there are three segmentation modalities:\n",
    "\n",
    "- **{ref}`New subject <newsubject>`**: Segmentation of single images, baseline images in longitudinal studies, or high-resolution images in multimodal acquisitions\n",
    "- **{ref}`Multimodal <multimodal>`**: Segmentation of images acquired with different protocols, where the highest resolution image has already been segmented as *new subject*\n",
    "- **{ref}`Longitudinal <longitudinal>`**: Segmentation of followup images, where the baseline image has already been segmented as *new subject*\n",
    "\n",
    "For the execution, the differences among the three modalities are:\n",
    "\n",
    "- The **reference image**\n",
    "- The structure of the **input file**\n",
    "- The variable ``modality`` in ``segmentation_sa.ipynb``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "(newsubject)=\n",
    "\n",
    "## New subject segmentation\n",
    "\n",
    "### Reference image\n",
    "\n",
    "The reference image is the same for all the images to be segmented\n",
    "\n",
    "In the folder ``reference``, the reference image is in the folder ``newsubject``, which contains:\n",
    "\n",
    "- ``reference.mha``: Reference image for the atlas-based segmentation\n",
    "- ``reference_f.mha``: Femur mask of the reference image\n",
    "- ``reference_fc.mha``: Femoral cartilage mask of the reference image.\n",
    "\n",
    "This reference was found with a {ref}`convergence study<findReference>` on 19 segmented images\n",
    "\n",
    "```{tip}\n",
    "When using **your own data**:\n",
    "- You can use the same reference image contained in the demo dataset: copy the files `reference.mha`, `reference_f.mha`, and `reference_fc.mha` to your `reference/newsubject` folder  \n",
    "- You can use a different reference image: copy your new reference image, femur mask, and femoral cartilage mask to the `reference/newsubject` folder, making sure you rename the files as `reference.mha`, `reference_f.mha`, and `reference_fc.mha`  \n",
    "- If you want to find a reference image from an already segmented dataset, you can run *pyKNEEr* {ref}`convergence study<findReference>`\n",
    "```\n",
    "\n",
    "### Input: Image list\n",
    "\n",
    "For the demo images, the input file is ``image_list_newsubject.txt``, which contains:\n",
    "\n",
    "      [1] ./reference/newsubject\n",
    "      [2] ./preprocessed/\n",
    "      [3] r reference.mha\n",
    "      [4] m 01_DESS_01_prep.mha\n",
    "\n",
    "where:\n",
    "\n",
    "- Line 1: Reference folder, containing the reference image and its masks\n",
    "- Line 2: Preprocessed file folder, containing the preprocessed files\n",
    "- Line 3: Reference image, indicated as ``r``\n",
    "- Line 4: Moving images, indicated as ``m``\n",
    "\n",
    "```{tip}\n",
    "When using **your own data**:  \n",
    "- Customize ``image_list_newsubject.txt`` with the paths and the names of your images\n",
    "- There is no limit to the number of moving ``m`` images\n",
    "```\n",
    "\n",
    "(execution)=\n",
    "### Executing `segmentation_sa_ns.ipynb`\n",
    "\n",
    "To segment the data:\n",
    "\n",
    "- {ref}`Launch <launch_jup>` Jupyter notebook\n",
    "- In *File Browser*, navigate to `segmentation_sa_ns.ipynb`, open it, and:\n",
    "  - Customize the input variable `n_of_cores` ({ref}`How do I choose the number of cores? <cores>`)\n",
    "  - Notice that variable `modality` is set to `newsubject`\n",
    "  - Follow the instructions in the notebook\n",
    "\n",
    "- {ref}`Save <save_jup>` your notebook at the end of the process\n",
    "\n",
    "(output)=\n",
    "\n",
    "### Output: Segmented images\n",
    "\n",
    "The masks are in the folder `segmented`. For each subjects, the outputs are:\n",
    "\n",
    "- `*_prep_fc.mha` (e.g. `01_DESS_01_prep_fc.mha`): Binary mask of the femoral cartilage\n",
    "- `*_prep_f.mha` (e.g. `01_DESS_01_prep_f.mha`): Binary mask of the femur, a byproduct of the registration\n",
    "\n",
    "\n",
    "```{note}\n",
    "\n",
    "- Intermediate registration steps are saved in the folder `registered`  \n",
    "- If you are not interested in analysis from deformations, you can delete the folder after your computations\n",
    "- If you want to compute further analysis, the folders `registered/subject_name` contain:  \n",
    "  - `fc_spline.mha` (intersubject and longitudinal segmentation) or `f_rigig.mha` (multimodal segmentation), which contain the moving image warped to the reference. They can be used for analysis such as voxel-based relaxometry  \n",
    "  - `TransformParameters.xxx.txt`, which contain transformation values. They can be used for PCA or other analysis. For their use, we forward to the <a href=\"https://github.com/SuperElastix/elastix/wiki/What-is-elastix/\" target=\"_blank\">elastix manual</a>\n",
    "```\n",
    "\n",
    "(visualization)=\n",
    "\n",
    "### Visualization: Superimposing cartilage mask onto the MR image\n",
    "\n",
    "For a qualitative check, for each subject we visualize three **2D** slices of the intensity image (`*_prep.mha`) overlapped by the corresponding slices of the cartilage mask (`*_prep_fc.mha`), similarly to this figure:\n",
    "\n",
    "\n",
    "![](images/newSubject.png)\n",
    "\n",
    "For a **3D** check, consider using a medical image software such as {ref}`ITK-SNAP <itksnap>`, which allows visualizing {ref}`the overlap of an image and its mask <itksnapMask>`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(multimodal)=\n",
    "\n",
    "## Multimodal segmentation\n",
    "\n",
    "\n",
    "###  Reference image\n",
    "\n",
    "For each acquisition at lower resolution (e.g. CubeQuant), the reference image is a **high-resolution image of the same subject** (e.g. DESS), which must have been previously {ref}`segmented as a newsubject <newsubject>`.\n",
    "\n",
    "In the folder `reference`, create the folder `multimodal`, and copy:\n",
    "\n",
    "- The high-resolution image: `01_DESS_01_prep.mha` from the folder `preprocessed`\n",
    "- The high-resolution femur mask: `01_DESS_01_prep_f.mha` from the folder `segmented`\n",
    "- The high-resolution femoral cartilage mask: `01_DESS_01_prep_fc.mha` from the folder `segmented`\n",
    "\n",
    "This step will be simplified in future versions of *pyKNEEr*\n",
    "\n",
    "```{tip}\n",
    "When using **your own data**:  \n",
    "- In the folder `reference` create the folder `multimodal`  \n",
    "- Copy the high-resolution images to be used as a references, together with their femur mask and femoral cartilage mask  \n",
    "```\n",
    "\n",
    "\n",
    "### Input: Image list\n",
    "\n",
    "For the demo images, the input file is `image_list_multimodal.txt`, which contains:  \n",
    "\n",
    "      [1] ./reference/multimodal\n",
    "      [2] ./preprocessed/\n",
    "      [3] r 01_DESS_01_prep.mha\n",
    "      [4] m 01_cubeQuant_01_prep.mha\n",
    "where:\n",
    "- Line 1: Reference folder, containing the baseline images used as reference\n",
    "- Line 2: Preprocessed file folder, containing the preprocessed images\n",
    "- Line 3: Reference (high res) image, indicated as ``r``\n",
    "- Line 4: Moving (low res) image, indicated as ``m``\n",
    "\n",
    "```{tip}\n",
    "When using **your own data**:  \n",
    "- Customize `image_list_multimodal.txt` with the paths and the names of your images\n",
    "- In case of several images to segment, write high-resolution images `r` and low-resolution images `m` in a coupled manner:\n",
    "\n",
    "        [1] ./reference/longitudinal\n",
    "        [2] ./preprocessed/\n",
    "        [3] r subject1_HRes_prep.mha\n",
    "        [4] m subject1_LRes_prep.mha\n",
    "        [5] r subject2_HRes_prep.mha\n",
    "        [6] m subject2_LRes_prep.mha\n",
    "        [7] r subject3_HRes_prep.mha\n",
    "        [8] m subject3_LRes_prep.mha\n",
    "        [9] etc.\n",
    "```\n",
    "\n",
    "### Execution, Output, and Visualization\n",
    "\n",
    "Execution:\n",
    "\n",
    "- To segment the data, apply the {ref}`instructions <execution>` above to the notebook `segmentation_sa_mm.ipynb`. Note that the variable `modality` is set to `multimodal`\n",
    "\n",
    "Output and visualization:\n",
    "\n",
    "- Follow the instructions above to know the {ref}`output <output>` and how to {ref}`visualize <visualization>` the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(longitudinal)=\n",
    "\n",
    "## Longitudinal\n",
    "\n",
    "For this segmentation modality, we do not provide a demo example but only instructions as it is very similar to multimodal segmentation\n",
    "\n",
    "\n",
    "### Reference image\n",
    "\n",
    "For each **followup** image, the reference image is the **corresponding baseline image**, which must have been previously segmented as a {ref}`new subject <newsubject>`\n",
    "\n",
    "In the folder `reference`, create the folder `longitudinal`, and for each image copy:  \n",
    "- The baseline image: `BL_prep.mha`  \n",
    "- The baseline femur mask: `BL_prep_f.mha`  \n",
    "- The baseline femoral cartilage mask: `BL_prep_fc.mha`  \n",
    "\n",
    "This step will be simplified in future versions of *pyKNEEr*.\n",
    "\n",
    "\n",
    "\n",
    "### Input: Image list\n",
    "\n",
    "Create the file `image_list_longitudinal.txt`, which will contain:\n",
    "\n",
    "      [1] ./reference/longitudinal\n",
    "      [2] ./preprocessed/\n",
    "      [3] r subject1_BL_prep.mha\n",
    "      [4] m subject1_FU_prep.mha\n",
    "      [5] r subject2_BL_prep.mha\n",
    "      [6] m subject2_FU_prep.mha\n",
    "      [7] r subject3_BL_prep.mha\n",
    "      [8] m subject3_FU_prep.mha\n",
    "\n",
    "where:  \n",
    "- Line 1: Reference folder, containing the the baseline images used as reference  \n",
    "- Line 2: Preprocessed file folder, containing the preprocessed files of the corresponding followup images\n",
    "- Odd lines from 3 to 7: Reference (baseline) images, indicated as `r`    \n",
    "- Even lines 4 to 8: Moving (followup) images, indicated as `m`  \n",
    "\n",
    "### Execution, Output, and Visualization\n",
    "\n",
    "Execution:\n",
    "- To segment the data, apply the {ref}`instructions <execution>` above. Set the variable `modality` to `longitudinal`\n",
    "\n",
    "Output and visualization:  \n",
    "- Follow the instructions above to know the {ref}`output <output>` and how to {ref}`visualize <visualization>` the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation Plus\n",
    "\n",
    "*pyKNEEr* includes notebooks to find the reference image and evaluate segmentation quality\n",
    "\n",
    "These two steps are not included in the demo for sake of simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(findReference)=\n",
    "### Finding reference image\n",
    "\n",
    "In this convergence study, the new reference is the image of the dataset whose vector field is the closest to the average of the vector fields of the dataset. The study runs until convergence or for a fixed amount of iterations\n",
    "\n",
    "```{note}\n",
    "   To run this convergence study all the images of the dataset **must already have** a **femur mask**\n",
    "}\n",
    "```\n",
    "\n",
    "The Jupyter notebook to find a reference image is <a href=\"https://github.com/sbonaretti/pyKNEEr/blob/master/pykneer/notebooks/find_reference.ipynb\" target=\"_blank\">find_reference.ipynb</a>\n",
    "\n",
    "\n",
    "#### Picking random seeds\n",
    "To determine the image that you are going to use as reference, we recommend a random generator function with a fixed seed to make the reference selection reproducible.\n",
    "The code is <a href=\"https://github.com/sbonaretti/pyKNEEr/tree/master/pykneer/pykneer/find_reference_random_gen.py\" target=\"_blank\">here</a>.\n",
    "  \n",
    "You can run several convergence study in parallel to confirm you find the same reference image independently from the starting seed\n",
    "\n",
    "\n",
    "#### Input: Image list\n",
    "\n",
    "Data required are MR images of the knee that have segmented femurs because:   \n",
    "1) the registration is guided by the femur mask     \n",
    "2) the average vector field is calculated in the femur mask  \n",
    "3) the comparison between the average vector field and each image vector field is performed in the femur mask\n",
    "\n",
    "In you data folder, create a folder called `findReference` and add the preprocessed images of the  dataset with their masks:\n",
    "\n",
    "\n",
    "    - subject1_prep.mha\n",
    "    - subject1_f.mha\n",
    "    - subject2_prep.mha\n",
    "    - subject2_f.mha\n",
    "    - subject3_prep.mha\n",
    "    - subject3_f.mha\n",
    "\n",
    "File nomenclature has be as follows:  \n",
    "- The file name root of image and corresponding mask has to be the same  \n",
    "- The image name has to end in `_prep.mha`  \n",
    "- The mask name must end in `_f.mha`  \n",
    "\n",
    "Create the input file:\n",
    "\n",
    "    [1] ./findReference\n",
    "    [2] r subject2_prep.mha\n",
    "    [3] m subject1_prep.mha\n",
    "    [4] m subject2_prep.mha\n",
    "    [5] m subject3_prep.mha\n",
    "    [6] m etc.\n",
    "\n",
    "where:\n",
    "- Line 1: findReference folder, containing all the images of the dataset  \n",
    "- Line 2: Reference image, indicated as `r`  \n",
    "- Line 3-5: Moving images, indicated as `m`  \n",
    "  \n",
    "Note that in this example `subject2_prep.mha` is both the reference and an image of the dataset, because we want to include it as a possible candidate for being the final reference\n",
    "\n",
    "If you run multiple studies, created the input file for every seed image, adapting the reference (`r`) file name\n",
    "\n",
    "\n",
    "#### Executing findReference.ipynb\n",
    "For each seed image, in `findReference.ipynb` customize `input_file_name` and `n_of_cores`\n",
    "\n",
    "Launch `findReference.ipynb`. It will run until convergence or until the number of iterations reaches 10 (If you run the source code, you can change the number of iterations in the file `find_reference_for_nb.py`, function `find_reference`, variable `maxIterationNo`)\n",
    "\n",
    "\n",
    "#### Output: Convergence plot\n",
    "\n",
    "The output of the computation is a convergence plot. The graph can reach a plateau or can be zig-zagged. In this last case, choose the reference with the lowest error (y-axis). If the graph shows less than 10 iterations it means that the current reference image is the same as the one in the previous loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(segmentationQuality)=\n",
    "\n",
    "### Segmentation quality\n",
    "\n",
    "You can quantify segmentation quality when a **ground truth** segmentation is present, whereas you can  evaluate segmentation quality only visually when a ground truth segmentation is not available\n",
    "\n",
    "The metrics we use to evaluate segmentation quality are:  \n",
    "- Measures of *overlap agreement*: Dice coefficient, Jaccard coefficient, and volume similarity, which quantify the overlap between ground truth segmentations and *pyKNEEr* segmentations  \n",
    "- Measure of *surface distance*: Average of the Euclidean distances between ground truth segmentations and *pyKNEEr* segmentations\n",
    "\n",
    "The Jupyter notebook to evaluate segmentation is <a href=\"https://github.com/sbonaretti/pyKNEEr/blob/master/pykneer/notebooks/segmentation_quality.ipynb\" target=\"_blank\">segmentation_quality.ipynb</a>\n",
    "\n",
    "\n",
    "#### Input: Image list\n",
    "\n",
    "Create the input file:\n",
    "\n",
    "    [1] ./segmented\n",
    "    [2] ./segmented_groundTruth\n",
    "    [3] s subject1_prep_fc.mha\n",
    "    [4] g subject1_groundTruth_fc.mha\n",
    "    [5] s subject2_prep_fc.mha\n",
    "    [6] g subject2_groundTruth_fc.mha\n",
    "    [7] m etc.\n",
    "\n",
    "where:\n",
    "\n",
    "- Line 1: Segmented folder, containing the masks obtained with *pyKNEEr*  \n",
    "- Line 2: Ground truth folder, containing ground truth masks  \n",
    "- Lines 3,5: Segmentations obtained with *pyKNEEr*, indicated as `s`  \n",
    "- Lines 4,6: Ground truth segmentations, indicated as `g`  \n",
    "\n",
    "\n",
    "#### Execution, Output, and Visualization\n",
    "\n",
    "Execution:\n",
    "\n",
    "- In `segmentation_quality.ipynb`, customize `input_file_name`, `output_file_name_overlap`, and `output_file_name_distances`, and execute\n",
    "\n",
    "Output and visualization:\n",
    "\n",
    "- Results will be visualized as graphs and tables, and will be saved in the `.csv` files for possible subsequent analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "(Klein)= \n",
    "[1] Klein S., Staring M., Murphy K., Viergever M.A., Pluim J.P.W.\n",
    "<a href=\"http://elastix.isi.uu.nl/marius/downloads/2010_j_TMI.pdf\" target=\"_blank\">\n",
    "<i>elastix: A Toolbox for Intensity-Based Medical Image Registration.</i></a>\n",
    "IEEE Transactions on Medical Imaging. vol. 29, no. 1, pp. 196 - 205, January. 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "````{panels}\n",
    "\n",
    "```{link-button} newsubject  <- tag \n",
    ":text: New subject           <- text\n",
    ":type: ref  \n",
    ":classes: stretched-link\n",
    "```\n",
    "---\n",
    "\n",
    "```{link-button} multimodal\n",
    ":text: Multimodal  \n",
    ":type: ref  \n",
    ":classes: stretched-link\n",
    "```\n",
    "---\n",
    "\n",
    "```{link-button} longitudinal\n",
    ":text: Longitudinal  \n",
    ":type: ref  \n",
    ":classes: stretched-link\n",
    "```\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
